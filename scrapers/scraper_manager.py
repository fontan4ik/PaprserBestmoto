"""
–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∫—Ä–∞–ø–µ—Ä–æ–≤
–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º–∏ Selenium —Å–∫—Ä–∞–ø–µ—Ä–∞–º–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
"""

from typing import List, Dict, Optional
import logging
import re
from dataclasses import dataclass, asdict

from .wildberries_scraper import WildberriesScraper
from .ozon_scraper import OzonScraper
from .avito_scraper import AvitoScraper
from .yandex_market_scraper import YandexMarketScraper
from .universal_scraper import UniversalScraper

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScrapedProduct:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–æ–≤–∞—Ä–∞"""
    title: str
    price: float
    old_price: Optional[float] = None
    url: str = ""
    source: str = ""
    availability: str = "unknown"
    brand: str = ""
    rating: float = 0.0
    reviews_count: int = 0
    image_url: str = ""
    location: str = ""
    seller: str = ""
    
    def to_dict(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return asdict(self)


class ScraperManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ—Ö —Å–∫—Ä–∞–ø–µ—Ä–æ–≤
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤
    """
    
    SUPPORTED_SITES = {
        'wildberries': 'Wildberries',
        'ozon': 'OZON',
        'avito': 'Avito',
        'yandex_market': '–Ø–Ω–¥–µ–∫—Å –ú–∞—Ä–∫–µ—Ç',
        'mr-moto': 'mr-moto.ru',
        'flipup': 'flipup.ru',
        'pro-ekip': 'pro-ekip.ru',
        'motoekip': 'motoekip.su',
        'motocomfort': 'motocomfort.ru',
    }
    
    SITE_ALIASES = {
        'mrmotoru': 'mr-moto',
        'flipupru': 'flipup',
        'proekipru': 'pro-ekip',
        'motoekipsu': 'motoekip',
        'motocomfortru': 'motocomfort',
    }
    
    def __init__(self, headless: bool = True):
        """
        Args:
            headless: –∑–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ headless —Ä–µ–∂–∏–º–µ
        """
        self.headless = headless
        self._scrapers = {}
        self.logger = logging.getLogger(__name__)
    
    def search_all(self, query: str, sites: Optional[List[str]] = None, max_products: int = 20) -> Dict[str, List[ScrapedProduct]]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞ –≤—Å–µ—Ö –∏–ª–∏ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö
        
        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            sites: —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ (–µ—Å–ª–∏ None - –ø–æ–∏—Å–∫ –Ω–∞ –≤—Å–µ—Ö)
            max_products: –º–∞–∫—Å–∏–º—É–º —Ç–æ–≤–∞—Ä–æ–≤ —Å –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞
        
        Returns:
            —Å–ª–æ–≤–∞—Ä—å {—Å–∞–π—Ç: [—Ç–æ–≤–∞—Ä—ã]}
        """
        if sites is None:
            sites = list(self.SUPPORTED_SITES.keys())
        
        results = {}
        
        for site in sites:
            canonical_site = self._normalize_site_key(site)
            if not canonical_site:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∞–π—Ç: {site}")
                continue
            
            try:
                products = self.search(canonical_site, query, max_products)
                results[canonical_site] = products
                self.logger.info(f"‚úÖ {canonical_site}: –Ω–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ {canonical_site}: {e}")
                results[canonical_site] = []
        
        return results
    
    def search(self, site: str, query: str, max_products: int = 20) -> List[ScrapedProduct]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Å–∞–π—Ç–µ
        
        Args:
            site: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_products: –º–∞–∫—Å–∏–º—É–º —Ç–æ–≤–∞—Ä–æ–≤
        
        Returns:
            —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
        """
        canonical_site = self._normalize_site_key(site)
        if not canonical_site:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∞–π—Ç: {site}")
            return []
        
        self.logger.info(f"üîç –ü–æ–∏—Å–∫ –Ω–∞ {canonical_site}: '{query}'")
        
        products = []
        
        try:
            if canonical_site == 'wildberries':
                products = self._search_wildberries(query, max_products)
            
            elif canonical_site == 'ozon':
                products = self._search_ozon(query, max_products)
            
            elif canonical_site == 'avito':
                products = self._search_avito(query, max_products)
            
            elif canonical_site == 'yandex_market':
                products = self._search_yandex_market(query, max_products)
            
            elif canonical_site in ['mr-moto', 'flipup', 'pro-ekip', 'motoekip', 'motocomfort']:
                domain = self.SUPPORTED_SITES[canonical_site]
                products = self._search_universal(domain, query, max_products)
            
            else:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∞–π—Ç: {canonical_site}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            unified_products = []
            for p in products:
                unified = ScrapedProduct(
                    title=p.title,
                    price=p.price,
                    old_price=getattr(p, 'old_price', None),
                    url=p.url,
                    source=p.source,
                    availability=getattr(p, 'availability', 'unknown'),
                    brand=getattr(p, 'brand', ''),
                    rating=getattr(p, 'rating', 0.0),
                    reviews_count=getattr(p, 'reviews_count', 0),
                    image_url=getattr(p, 'image_url', ''),
                    location=getattr(p, 'location', ''),
                    seller=getattr(p, 'seller', ''),
                )
                unified_products.append(unified)
            
            return unified_products
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ {canonical_site}: {e}")
            return []
    
    def _get_scraper(self, scraper_name: str, scraper_class):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∞–ø–µ—Ä (—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)"""
        if scraper_name not in self._scrapers:
            self._scrapers[scraper_name] = scraper_class(headless=self.headless)
        return self._scrapers[scraper_name]
    
    def _search_wildberries(self, query: str, max_products: int) -> List:
        """–ü–æ–∏—Å–∫ –Ω–∞ Wildberries"""
        scraper = self._get_scraper('wildberries', WildberriesScraper)
        return scraper.search(query, max_products)
    
    def _search_ozon(self, query: str, max_products: int) -> List:
        """–ü–æ–∏—Å–∫ –Ω–∞ OZON"""
        scraper = self._get_scraper('ozon', OzonScraper)
        return scraper.search(query, max_products)
    
    def _search_avito(self, query: str, max_products: int) -> List:
        """–ü–æ–∏—Å–∫ –Ω–∞ Avito"""
        scraper = self._get_scraper('avito', AvitoScraper)
        return scraper.search(query, max_products)
    
    def _search_yandex_market(self, query: str, max_products: int) -> List:
        """–ü–æ–∏—Å–∫ –Ω–∞ –Ø–Ω–¥–µ–∫—Å –ú–∞—Ä–∫–µ—Ç"""
        scraper = self._get_scraper('yandex_market', YandexMarketScraper)
        return scraper.search(query, max_products)
    
    def _search_universal(self, site: str, query: str, max_products: int) -> List:
        """–ü–æ–∏—Å–∫ –Ω–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö"""
        scraper_key = f'universal_{site}'
        if scraper_key not in self._scrapers:
            self._scrapers[scraper_key] = UniversalScraper(headless=self.headless)
        scraper = self._scrapers[scraper_key]
        return scraper.search(site, query, max_products)
    
    def close_all(self):
        """–ó–∞–∫—Ä—ã—Ç—å –≤—Å–µ –±—Ä–∞—É–∑–µ—Ä—ã"""
        for name, scraper in self._scrapers.items():
            try:
                scraper.close()
                self.logger.info(f"üîí –ó–∞–∫—Ä—ã—Ç —Å–∫—Ä–∞–ø–µ—Ä: {name}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è {name}: {e}")
        self._scrapers.clear()
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä - –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –±—Ä–∞—É–∑–µ—Ä—ã"""
        try:
            self.close_all()
        except:
            pass
    
    def get_supported_sites(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Å–∞–π—Ç–æ–≤"""
        return self.SUPPORTED_SITES.copy()
    
    def _normalize_site_key(self, site: Optional[str]) -> Optional[str]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥—è—â–µ–µ –∏–º—è —Å–∞–π—Ç–∞ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–º—É –∫–ª—é—á—É"""
        if not site:
            return None
        
        if site in self.SUPPORTED_SITES:
            return site
        
        sanitized = re.sub(r'[\s\-.]', '', site.lower())
        alias = self.SITE_ALIASES.get(sanitized)
        if alias:
            return alias
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–±—É–µ–º —É–±—Ä–∞—Ç—å www
        if sanitized.startswith('www'):
            alias = self.SITE_ALIASES.get(sanitized[3:])
            if alias:
                return alias
        
        return None


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    print("=" * 80)
    print("  –¢–ï–°–¢ SCRAPER MANAGER")
    print("=" * 80)
    print()
    
    manager = ScraperManager(headless=True)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å–∞–π—Ç—ã
    print("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å–∞–π—Ç—ã:")
    for key, name in manager.get_supported_sites().items():
        print(f"  - {key}: {name}")
    
    print("\n" + "=" * 80)
    print("  –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫")
    print("=" * 80)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–∞—Ö
    query = "–º–æ—Ç–æ—à–ª–µ–º HJC"
    sites = ['wildberries', 'ozon']  # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ 2 —Å–∞–π—Ç–∞
    
    print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
    print(f"üìç –°–∞–π—Ç—ã: {', '.join(sites)}")
    print()
    
    results = manager.search_all(query, sites=sites, max_products=3)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for site, products in results.items():
        print(f"\n{'='*80}")
        print(f"  {site.upper()}: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"{'='*80}")
        
        if products:
            for i, product in enumerate(products, 1):
                print(f"\n{i}. {product.title[:60]}...")
                print(f"   üí∞ –¶–µ–Ω–∞: {product.price:,.0f}‚ÇΩ")
                if product.old_price:
                    print(f"   üí∏ –ë—ã–ª–æ: {product.old_price:,.0f}‚ÇΩ")
                if product.rating > 0:
                    print(f"   ‚≠ê –†–µ–π—Ç–∏–Ω–≥: {product.rating}")
                print(f"   üîó {product.url}")
        else:
            print("\n‚ö†Ô∏è –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print("\n" + "=" * 80)
    print("  ‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print("=" * 80)
